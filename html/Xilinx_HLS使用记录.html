<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN" data-theme="tufte">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="description" content="Xilinx_HLS上板过程记录" />
  <meta name="keywords" content="xilinx hls, high level synthesis, fpga, zynq, opencl" />
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="/extra/classless.css" />
<style>
html[data-theme='tufte']{
    --rem: 15px;
    --navpos: absolute;
    --width: 70rem;
    --font-p: 1.4rem/2 "LXGW Bright", Georgia, serif;
    --font-h: 1.4rem/1.5 "LXGW Bright", Georgia, serif;
    --font-c: .9em/1.4 Consolas,"Liberation Mono","Courier New",monospace;
    --ornament: "";
    --border: 1px solid var(--cmed);
    /* foreground   | background color */
    --cfg:   #111;    --cbg:    #fbf7ec;
    --cdark: #111;    --clight: #fbf7ec;
    --cmed: #b4d5fe;
    --clink: #111;
    --cemph: #111;
}
</style>
  <script>
      let search = () => {
          let word = document.querySelector('input');
          if (word.value.length == 0) {
              word.focus();
          } else {
              location.href = '/search.html?' + word.value;
          }
      };
      window.onload = () => {
          document.querySelector('input').addEventListener(
              'keydown',  (event) => {
                  if (event.key == 'Enter') search();
          });
      }
  </script>
  <link rel="icon" href="/img/favicon.ico" type="image/x-icon" />
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" type="text/css" href="https://chinese-fonts-cdn.deno.dev/packages/lxgwwenkaibright/dist/LXGWBright-Regular/result.css"/>
  <title>Xilinx_HLS上板过程记录</title>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
    <nav>
        <ul>
            <li><big>VnYzm的博客</big></li>
            <li><a href="/index.html">技术</a></li>
            <li><a href="/miscellany.html">杂谈</a></li>
            <li><a href="/friends.html">友链</a></li>
            <li><a href="/html/关于.html">关于</a></li>
            <li><a href="https://www.travellings.cn/go.html" target="_blank">开往</a></li>
            <li class="float-right sticky"><button style="margin: 0" onclick="search()">
              <svg width="1em" height="1em" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"> <path d="M15.7955 15.8111L21 21M18 10.5C18 14.6421 14.6421 18 10.5 18C6.35786 18 3 14.6421 3 10.5C3 6.35786 6.35786 3 10.5 3C14.6421 3 18 6.35786 18 10.5Z" stroke="#000000" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path> </g></svg>
            </button></li>
            <li class="float-right sticky"><input type="search" placeholder="搜索标题" style="margin: 0"></li>
        </ul>
    </nav>
<header id="title-block-header">
<h1 class="title">Xilinx_HLS上板过程记录</h1>
<hr>
</header>
<h3 id="背景">背景</h3>
<p>最近做一个FPGA加速项目，懒得写RTL，所以又选择了HLS（High Level Synthesis，高层次综合）。之前的文章《Ultra96V2开发板简单使用》中介绍了如何用HLS写IP核并且在Ultra96V2开发板上通过Pynq环境跑起来，但是我现在用的是OpenSSD开发板，如《SpinalHDL上板过程记录》说的，虽然也是Zynq系列的但是没有官方的带PYNQ的操作系统镜像，而是直接用Vitis通过JTAG把运行时和硬件比特流一起传到没有操作系统的ARM核上跑，在这个过程中就出现了一些两篇文章中都没有出现过的问题，所以做个记录。</p>
<p>此外，项目还需要在SmartSSD上跑，这个环境下是通过X86的主机控制FPGA设备。在这个环境上面部署HLS也会遇到一些问题，所以在这里一并记录。</p>
<h3 id="zynq裸机">Zynq裸机</h3>
<h4 id="寄存器api">寄存器API</h4>
<p>Vitis里不能像Pynq里那样直接用参数名来控制AXILite寄存器。所幸HLS在打包IP核的时候会自动生成一套控制该IP核的源代码。假设HLS代码的主函数名是IPmain，则在Export RTL之后，即可在HLS项目目录下的<code>solution名\impl\ip\drivers\IPmain_v1_0\src</code>里找到，把除了<code>Makefile</code>之外的所有文件复制到Vitis的C/C++源代码目录下，这样就能在Host程序里调用IP核初始化、读写寄存器等操作的函数了，具体可以查看这些文件里面包含的内容。</p>
<h4 id="ip核的启动">IP核的启动</h4>
<p>在SpinalHDL里，可以通过定义AXIlite接口的onWrite事件来触发IP核的启动（准确来说是让硬件的状态机从idle状态变为工作状态），但在HLS则没法这样做。因此如果没有启动控制信号，硬件就会一直在参数未知的情况下不停地执行，如果是组合逻辑电路，则状态更是无法确定。为此需要显示地指定一个启动信号，可以额外定义一个布尔参数<code>start</code>，然后在HLS程序里显示写明：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1"></a><span class="cf">if</span> (start) {</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="co">// 实际工作代码</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>}</span></code></pre></div>
<p>此外，在Host端传完参数后要在打开<code>start</code>信号之后立刻关闭，保证硬件在执行完当前参数指定的任务就处在idle状态，而不是一直执行下去。具体来说是这样：</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1"></a>XTime before, after;</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="co">// 传别的参数</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>XIPmain_Set_start_r(IPmain, <span class="kw">true</span>);</span>
<span id="cb2-4"><a href="#cb2-4"></a>XIPmain_Set_start_r(IPmain, <span class="kw">false</span>);</span>
<span id="cb2-5"><a href="#cb2-5"></a>XTime_GetTime(&amp;before);</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co">// 硬件开始执行</span></span></code></pre></div>
<h4 id="执行时间的测量">执行时间的测量</h4>
<p>开始执行的时刻我们已经明确了，就是打开start信号的那一刻，那么结束执行的时刻呢？一个直观的想法就是在HLS里定义一个end变量，当start为真时，end设置为假，然后在代码的最后再设置为真，Host检测到end为真的时刻就是结束时刻。然而，这种方法是不行的，因为HLS到底是按软件的思路去编译的，这样搞编译器看到end变量函数返回时必定为false，就会生成一个<code>assign end = false</code>的组合电路了，显然不符合我们的心意。正确的方法是利用HLS给每个输出参数（也就是参数传递时传的是引用的参数）生成的<code>o_vld</code>信号。虽然end一直是false，但它的<code>o_vld</code>信号确实是等到它被赋值之后才变成真，具体而言，在Host需要做的事就是：</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1"></a><span class="co">// 传参数</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="cf">while</span> (!XIPmain_Get_end_o_vld(IPmain));</span>
<span id="cb3-3"><a href="#cb3-3"></a>XTime_GetTime(&amp;after);</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="bu">std::</span>cout &lt;&lt; (<span class="dt">double</span>)((u64)after - (u64)before) / COUNTS_PER_SECOND; <span class="co">// 输出执行时间</span></span></code></pre></div>
<p>注意当<code>o_vld</code>为真时被读了一次之后就会立刻变为假。此外如果IP核有别的输出参数，也可以不用专门定义一个<code>end</code>参数，用这个参数的<code>o_vld</code>就行，只要保证这个参数的赋值时刻是在任务执行代码的末尾即可。</p>
<h4 id="缓存刷新">缓存刷新</h4>
<p>什么时候需要刷新缓存？这个问题不是只有写HLS才会遇到的，而是只要在没有操作系统的ARM核上跑硬件都会遇到的，所以这里也记录一下。只需要记住一点，<strong>刷新缓存就是把缓存里的东西写到DRAM里，并将缓存标记为无效</strong>。标记为无效的意思就是下次Host端再要读取数据，就必须从DRAM里读取而不能从缓存里读取了。我遇到的一般有两个地方需要刷新缓存：</p>
<ul>
<li>Host端写完数据，接下来PL端要读的时候。因为Host写数据都是写到缓存，这时就要将缓存写到DRAM里，PL端才能读。</li>
<li>Host端读完数据，然后PL端修改了这些数据，接下来Host端又要读的时候。因为Host读数据的时候也会把数据读入缓存，之后PL端修改了DRAM但缓存没有被修改，这时Host再从缓存读出来的数据就是错的。因此需要<strong>在PL端修改数据前</strong>刷新缓存，让缓存无效。</li>
</ul>
<p>我刷新缓存用的是<code>Xil_DCacheFlush</code>，本来按理来说是应该用可以指定刷新地址区域的函数的，但不知道为什么我测出来的时间是用上面这个刷新整个DRAM的函数更快。可能是因为我的板子DRAM比较小，只有4G，刷新整个DRAM的额外开销比找地址的额外开销还要小吧。</p>
<h3 id="x86主机">X86主机</h3>
<p>SmartSSD（集成FPGA和SSD的智能存储设备）是以板卡的形式通过PCIE接口连接到装有操作系统的X86主机。Xilinx为这种平台提供了两种控制FPGA的API：XRT Native API和OpenCL API，我用的是OpenCL API。这个平台下就不需要给HLS代码添加前面说的那些内容，因为有显式的启动硬件的函数。具体可以参考<a href="https://docs.amd.com/r/en-US/ug1393-vitis-application-acceleration/OpenCL-Programming">Xilinx的UG1393文档</a>。</p>
<h4 id="hls主函数参数">HLS主函数参数</h4>
<p>OpenCL平台下的HLS主函数参数不需要显式的指定参数通过AXILite控制，即不需要<code>#pragma HLS INTERFACE mode=s_axilite port=</code>这种，只需要指定通过AXI传输的参数即<code>#pragma HLS INTERFACE mode=m_axi port=</code>这种。虽然不需要指定，但实际在实现的时候这些参数还是通过AXILite控制的，而且还是单工的AXILite，只能Host往硬件传参，不能硬件往Host返回，所以普通HLS代码里的引用传参就被ban了，想往外传数据只能通过DRAM。</p>
<p>此外有一个非常坑爹的地方，那就是非指针参数的类型<strong>不能是ap_[u]int</strong>类型，这个地方坑了我特别久，也不报错，就是传参的时候给你传个残缺的数据，你说恶不恶心！非指针参数的类型只能是C/C++的基础类型，包括bool、int、long这些。不过指针参数是可以的，也就是说可以声明<code>ap_int&lt;128&gt;*</code>这种。另外和普通FPGA平台不同，普通平台上HLS代码定义了这样的指针类型打包出来IP核的AXI总线位宽就是128位，如果是老的FPGA芯片（比如Zynq7000系列的）不支持这么宽的AXI协议，就会出问题，而这个平台上就没事，OpenCL会自动进行适配。</p>
<h4 id="数据传输">数据传输</h4>
<p>和Zynq平台不同，这个平台下Host和FPGA的DRAM不是共享的。OpenCL提供了两种分配（主机）缓冲区的方法，详见UG1393。Using Host Pointer Buffers的方法没啥坑点，而Letting XRT Allocate Buffers却有陷阱。按照文档说的，先<code>clCreateBuffer</code>创建缓冲区，然后<code>clEnqueueMapBuffer</code>获取Host端的指针，有的人可能以为这个函数和Linux的<code>mmap</code>一样是把Host的内存区域映射到Device，因此只需要映射一次，之后读写这段内存区域就是读写Device的数据了。然而这个想法是错的，如果是往设备写数据，<code>clEnqueueMapBuffer</code>获取指针并写入后<strong>需要执行</strong><code>clEnqueueMigrateMemObject</code><strong>才能将主机的数据传到设备。</strong> 如果是从设备读数据，可以不调用<code>clEnqueueMigrateMemObject</code>，但那样的话每次读的时候<strong>都需要重新执行</strong><code>clEnqueueMapBuffer</code><strong>重新获取指针并将设备的数据传回主机。</strong> 这点官方文档没有强调，导致很多人犯迷糊，我也是做了实验才明白这一点。</p>
<p>从上面可以看出，不管是哪种分配缓冲区的方法，都需要在Host端和FPGA设备端之间倒腾数据，区别只是在于用<code>clEnqueueMigrateMemObject</code>还是<code>clEnqueueMapBuffer</code>（获取设备端数据的时候），倒腾的过程总是有时间开销的。有没有不需要倒腾的方法吗？有，Xilinx扩展OpenCL，提供了一个叫P2P Buffer的东西，<a href="https://github.com/FFGGSSJJ/SmartSSD-Oriented-Work/tree/main/datatransfer/SSD-FPGA/p2p_bandwidth">这个代码仓库</a>给出了示例，简而言之就是在<code>clCreateBuffer</code>的时候添加<code>CL_MEM_EXT_PTR_XILINX</code>标示位，并传一个扩展指针进该函数。然后就像前面所预想的，只需要用<code>clEnqueueMapBuffer</code>映射一次，之后读写这段内存区域就是读写Device的数据了。不过正因为是直接读取FPGA设备上DRAM的数据，传到Host的开销还是客观存在的，因此如果读写大量数据效率会非常低，所以要读写大量数据还是用前面说的方法，直接一次性倒腾。当然如果是读写SmartSSD中集成的SSD里的文件到P2P缓冲区，这就非常快了，因为这两种的传输是不需要经过Host的。</p>
<h3 id="hls使用技巧">HLS使用技巧</h3>
<p>这里再补充三个HLS的使用技巧吧。</p>
<h4 id="编译期计算log2">编译期计算log2</h4>
<p>写硬件代码的时候经常有计算一个常量整数的<span class="math inline">\(log_2\)</span>的需求，SpinalHDL提供了对应的内建函数，但HLS居然没有提供。而且为了不生成多余的电路，我们需要在编译期进行计算，才能保证生成的RTL代码中结果也是以常数的形式表示。所幸我用的是C++：</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">template</span> &lt;<span class="dt">int</span> N, <span class="dt">int</span> P = <span class="dv">0</span>&gt;</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="kw">struct</span> log2i {</span>
<span id="cb4-3"><a href="#cb4-3"></a>    <span class="at">static</span> <span class="kw">constexpr</span> <span class="dt">int</span> value = log2i&lt;N / <span class="dv">2</span>, P + <span class="dv">1</span>&gt;::value;</span>
<span id="cb4-4"><a href="#cb4-4"></a>};</span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="kw">template</span> &lt;<span class="dt">int</span> P&gt;</span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="kw">struct</span> log2i&lt;<span class="dv">1</span>, P&gt; {</span>
<span id="cb4-7"><a href="#cb4-7"></a>    <span class="at">static</span> <span class="kw">constexpr</span> <span class="dt">int</span> value = P;</span>
<span id="cb4-8"><a href="#cb4-8"></a>};</span></code></pre></div>
<p>这里用到了模板元编程，能够在编译期计算常量整数取2对数的结果。不过我这里是向下取整，现实需求还是向上取整的比较多，还好我只把它用在输入整数刚好是2的幂的情况，所以怎么取整无所谓。</p>
<h4 id="dram和bram之间的数据传输">DRAM和BRAM之间的数据传输</h4>
<p>当DRAM的位宽和BRAM的位宽不一样时，两者之间的数据传输就是个比较麻烦的问题。这里记录一下我用模板元编程写的（实际AI帮了我很多😋)比较通用的代码：</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb5-1"><a href="#cb5-1"></a><span class="pp">#include </span><span class="im">&lt;type_traits&gt;</span></span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="kw">template</span>&lt;<span class="dt">int</span> buffer_width, <span class="dt">int</span> bus_width, <span class="kw">typename</span> <span class="bu">std::</span>enable_if&lt;bus_width &gt;= buffer_width, <span class="dt">int</span>&gt;::type = <span class="dv">0</span>&gt;</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="dt">void</span> load_dram(<span class="dt">addr_t</span> address, <span class="dt">addr_t</span> size, <span class="dt">addr_t</span> buffer_offset, ap_uint&lt;buffer_width&gt; *buffer, ap_uint&lt;bus_width&gt; *dram) {</span>
<span id="cb5-5"><a href="#cb5-5"></a>    load_wide_dram_ij:</span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="cf">for</span> (<span class="dt">addr_t</span> i = <span class="dv">0</span>, j = <span class="dv">0</span>; i &lt; size / (bus_width / <span class="dv">8</span>); i++, j += bus_width / buffer_width) {</span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>        ap_uint&lt;bus_width&gt; data = dram[address / (bus_width / <span class="dv">8</span>) + i];</span>
<span id="cb5-9"><a href="#cb5-9"></a>        load_wide_dram_k:</span>
<span id="cb5-10"><a href="#cb5-10"></a>        <span class="cf">for</span> (<span class="dt">addr_t</span> k = <span class="dv">0</span>; k &lt; bus_width / buffer_width; k++) {</span>
<span id="cb5-11"><a href="#cb5-11"></a>            buffer[buffer_offset + j + k] = data((k + <span class="dv">1</span>) * buffer_width - <span class="dv">1</span>, k * buffer_width);</span>
<span id="cb5-12"><a href="#cb5-12"></a>        }</span>
<span id="cb5-13"><a href="#cb5-13"></a>    }</span>
<span id="cb5-14"><a href="#cb5-14"></a>}</span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="kw">template</span>&lt;<span class="dt">int</span> buffer_width, <span class="dt">int</span> bus_width, <span class="kw">typename</span> <span class="bu">std::</span>enable_if&lt;bus_width &lt; buffer_width, <span class="dt">int</span>&gt;::type = <span class="dv">0</span>&gt;</span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="dt">void</span> load_dram(<span class="dt">addr_t</span> address, <span class="dt">addr_t</span> size, <span class="dt">addr_t</span> buffer_offset, ap_uint&lt;buffer_width&gt; *buffer, ap_uint&lt;bus_width&gt; *dram) {</span>
<span id="cb5-18"><a href="#cb5-18"></a>    load_wide_buffer_ij:</span>
<span id="cb5-19"><a href="#cb5-19"></a>    <span class="cf">for</span> (<span class="dt">addr_t</span> i = <span class="dv">0</span>, j = <span class="dv">0</span>; i &lt; size / (buffer_width / <span class="dv">8</span>); i++, j += buffer_width / bus_width) {</span>
<span id="cb5-20"><a href="#cb5-20"></a>        ap_uint&lt;buffer_width&gt; data;</span>
<span id="cb5-21"><a href="#cb5-21"></a>        load_wide_buffer_k:</span>
<span id="cb5-22"><a href="#cb5-22"></a><span class="pp">#pragma HLS UNROLL</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>        <span class="cf">for</span> (<span class="dt">addr_t</span> k = <span class="dv">0</span>; k &lt; buffer_width / bus_width; k++) {</span>
<span id="cb5-24"><a href="#cb5-24"></a>            data((k + <span class="dv">1</span>) * bus_width - <span class="dv">1</span>, k * bus_width) = dram[address / (bus_width / <span class="dv">8</span>) + j + k];</span>
<span id="cb5-25"><a href="#cb5-25"></a>        }</span>
<span id="cb5-26"><a href="#cb5-26"></a>        buffer[buffer_offset + i] = data;</span>
<span id="cb5-27"><a href="#cb5-27"></a>    }</span>
<span id="cb5-28"><a href="#cb5-28"></a>}</span>
<span id="cb5-29"><a href="#cb5-29"></a></span>
<span id="cb5-30"><a href="#cb5-30"></a><span class="kw">template</span>&lt;<span class="dt">int</span> buffer_width, <span class="dt">int</span> bus_width, <span class="kw">typename</span> <span class="bu">std::</span>enable_if&lt;bus_width &gt;= buffer_width, <span class="dt">int</span>&gt;::type = <span class="dv">0</span>&gt;</span>
<span id="cb5-31"><a href="#cb5-31"></a><span class="dt">void</span> store_dram(<span class="dt">addr_t</span> address, <span class="dt">addr_t</span> size, <span class="dt">addr_t</span> buffer_offset, ap_uint&lt;buffer_width&gt; *buffer, ap_uint&lt;bus_width&gt; *dram) {</span>
<span id="cb5-32"><a href="#cb5-32"></a>    store_wide_dram_ij:</span>
<span id="cb5-33"><a href="#cb5-33"></a>    <span class="cf">for</span> (<span class="dt">addr_t</span> i = <span class="dv">0</span>, j = <span class="dv">0</span>; i &lt; size / (bus_width / <span class="dv">8</span>); i++, j += bus_width / buffer_width) {</span>
<span id="cb5-34"><a href="#cb5-34"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb5-35"><a href="#cb5-35"></a>        ap_uint&lt;bus_width&gt; data;</span>
<span id="cb5-36"><a href="#cb5-36"></a>        store_wide_dram_k:</span>
<span id="cb5-37"><a href="#cb5-37"></a>        <span class="cf">for</span> (<span class="dt">addr_t</span> k = <span class="dv">0</span>; k &lt; bus_width / buffer_width; k++) {</span>
<span id="cb5-38"><a href="#cb5-38"></a>            data((k + <span class="dv">1</span>) * buffer_width - <span class="dv">1</span>, k * buffer_width) = buffer[buffer_offset + j + k];</span>
<span id="cb5-39"><a href="#cb5-39"></a>        }</span>
<span id="cb5-40"><a href="#cb5-40"></a>        dram[address / (bus_width / <span class="dv">8</span>) + i] = data;</span>
<span id="cb5-41"><a href="#cb5-41"></a>    }</span>
<span id="cb5-42"><a href="#cb5-42"></a>}</span>
<span id="cb5-43"><a href="#cb5-43"></a></span>
<span id="cb5-44"><a href="#cb5-44"></a><span class="kw">template</span>&lt;<span class="dt">int</span> buffer_width, <span class="dt">int</span> bus_width, <span class="kw">typename</span> <span class="bu">std::</span>enable_if&lt;bus_width &lt; buffer_width, <span class="dt">int</span>&gt;::type = <span class="dv">0</span>&gt;</span>
<span id="cb5-45"><a href="#cb5-45"></a><span class="dt">void</span> store_dram(<span class="dt">addr_t</span> address, <span class="dt">addr_t</span> size, <span class="dt">addr_t</span> buffer_offset, ap_uint&lt;buffer_width&gt; *buffer, ap_uint&lt;bus_width&gt; *dram) {</span>
<span id="cb5-46"><a href="#cb5-46"></a>    store_wide_buffer_ij:</span>
<span id="cb5-47"><a href="#cb5-47"></a>    <span class="cf">for</span> (<span class="dt">addr_t</span> i = <span class="dv">0</span>, j = <span class="dv">0</span>; i &lt; size / (buffer_width / <span class="dv">8</span>); i++, j += buffer_width / bus_width) {</span>
<span id="cb5-48"><a href="#cb5-48"></a><span class="pp">#pragma HLS PIPELINE</span></span>
<span id="cb5-49"><a href="#cb5-49"></a>        ap_uint&lt;buffer_width&gt; data = buffer[buffer_offset + i];</span>
<span id="cb5-50"><a href="#cb5-50"></a>        store_wide_buffer_k:</span>
<span id="cb5-51"><a href="#cb5-51"></a>        <span class="cf">for</span> (<span class="dt">addr_t</span> k = <span class="dv">0</span>; k &lt; buffer_width / bus_width; k++) {</span>
<span id="cb5-52"><a href="#cb5-52"></a>            dram[address / (bus_width / <span class="dv">8</span>) + j + k] = data((k + <span class="dv">1</span>) * bus_width - <span class="dv">1</span>, k * bus_width);</span>
<span id="cb5-53"><a href="#cb5-53"></a>        }</span>
<span id="cb5-54"><a href="#cb5-54"></a>    }</span>
<span id="cb5-55"><a href="#cb5-55"></a>}</span></code></pre></div>
<p>这里用了<code>std::enable_if</code>，本来如果用<code>if constexpr</code>可以简单很多的，可惜HLS编译器不支持C++17，所以只能借助SFINAE去选择编译。此外第二个<code>load_dram</code>函数里没用<code>#pragma HLS PIPELINE</code>，原因是用了之后综合好像会报错，我也不知道为什么，是不是编译器抽风了。</p>
<h4 id="双缓冲">双缓冲</h4>
<p>双缓冲是硬件设计里一项非常重要的技术。然而，如果在HLS直接这样写：</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb6-1"><a href="#cb6-1"></a><span class="dt">int</span> buffer[<span class="dv">2</span>][<span class="dv">100</span>];</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="cf">for</span> (<span class="dt">int</span> i = <span class="dv">0</span>, flag = <span class="kw">false</span>; i &lt; n; i++, flag = !flag) {</span>
<span id="cb6-3"><a href="#cb6-3"></a>    <span class="cf">if</span> (flag == <span class="dv">0</span>) {</span>
<span id="cb6-4"><a href="#cb6-4"></a>        <span class="co">// 读buffer[0]</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>        <span class="co">// 写buffer[1]</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>    } <span class="cf">else</span> {</span>
<span id="cb6-7"><a href="#cb6-7"></a>        <span class="co">// 读buffer[1]</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>        <span class="co">// 写buffer[0]</span></span>
<span id="cb6-9"><a href="#cb6-9"></a>    }</span>
<span id="cb6-10"><a href="#cb6-10"></a>}</span></code></pre></div>
<p>这样循环是流水不起来的，原因是编译器不知道buffer[0]和buffer[1]两者之间不存在依赖关系。好在看到了<a href="https://zhuanlan.zhihu.com/p/484130709">这篇文章</a>，关键思路在于通过函数来告诉编译器这两个buffer不会互相依赖：</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb7-1"><a href="#cb7-1"></a><span class="dt">void</span> read(<span class="dt">int</span> buffer[<span class="dv">100</span>]) {</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="pp">#pragma HLS ALLOCATION function instances=read limit=1</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="co">// 读buffer</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>}</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="dt">void</span> write(<span class="dt">int</span> buffer[<span class="dv">100</span>]) {</span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="pp">#pragma HLS ALLOCATION function instances=write limit=1</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>    <span class="co">// 写buffer</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>}</span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="co">// ...</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="dt">int</span> buffer[<span class="dv">2</span>][<span class="dv">100</span>];</span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="pp">#pragma HLS ARRAY_PARTITION dim=1 type=complete variable=buffer</span></span>
<span id="cb7-12"><a href="#cb7-12"></a></span>
<span id="cb7-13"><a href="#cb7-13"></a><span class="cf">for</span> (<span class="dt">int</span> i = <span class="dv">0</span>, flag = <span class="kw">false</span>; i &lt; n; i++, flag = !flag) {</span>
<span id="cb7-14"><a href="#cb7-14"></a>    <span class="cf">if</span> (flag == <span class="dv">0</span>) {</span>
<span id="cb7-15"><a href="#cb7-15"></a>        read(buffer[<span class="dv">0</span>]);</span>
<span id="cb7-16"><a href="#cb7-16"></a>        write(buffer[<span class="dv">1</span>]);</span>
<span id="cb7-17"><a href="#cb7-17"></a>    } <span class="cf">else</span> {</span>
<span id="cb7-18"><a href="#cb7-18"></a>        read(buffer[<span class="dv">1</span>]);</span>
<span id="cb7-19"><a href="#cb7-19"></a>        write(buffer[<span class="dv">0</span>]);</span>
<span id="cb7-20"><a href="#cb7-20"></a>    }</span>
<span id="cb7-21"><a href="#cb7-21"></a>}</span></code></pre></div>
<p>注意上面代码中的两个<code>#pragma</code>都非常重要：<code>#pragma HLS ALLOCATION function</code>告诉HLS这个函数只综合一套电路，强调了它的分时复用，也就暗示了双缓冲不会同时被读写；<code>#pragma HLS ARRAY_PARTITION</code>告诉HLS把buffer按第一维切分成完全不相交的BRAM，进一步突出双缓冲彼此独立。有了这两个<code>#pragma</code>，循环就能流水起来了。</p>
<h3 id="总结">总结</h3>
<p>之前我觉得HLS很难排除比较复杂的依赖关系，某些并行很难实现。但经过这次我才了解到如果按照某些范式来写程序（比如上面的用函数来解除依赖），还是可以实现比较精细的控制的。毕竟HLS写起来确实是比RTL简单太多，坑再怎么多写的时候确实爽。不过我这次写的程序比较简单，如果是复杂的程序，要查波形（虽然HLS编译器基本能百分之百保证程序功能的正确性，但硬件环境千变万化，接口方面还是有出错的可能，需要根据波形查错）、调时序，HLS似乎还是会力不从心。</p>
<footer>
    <hr>
    <center>
        Powered by <a href="https://github.com/ZimingYuan/Yblogs" target="_blank">Yblogs</a><br>
        字体：<a href="https://github.com/lxgw/LxgwWenKai" target="_blank">霞鹜文楷</a><br>
        转载请注明出处<br>
        © 2025 VnYzm的博客
    </center>
</footer>
</body>
</html>
